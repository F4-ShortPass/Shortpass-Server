# server/services/feedback/feedback_manager.py
"""
Feedback Manager - ì˜¤ë‹µë…¸íŠ¸ í”¼ë“œë°± ë£¨í”„ í•µì‹¬ ì„œë¹„ìŠ¤
HRì˜ í‰ê°€ ìˆ˜ì •ì„ ì €ì¥í•˜ê³ , ë‹¤ìŒ í‰ê°€ ì‹œ ìœ ì‚¬í•œ ì‹¤ìˆ˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•œ RAG ê²€ìƒ‰
"""
from typing import List, Dict, Optional
from sqlalchemy.orm import Session
from sqlalchemy import text
from openai import AsyncOpenAI
from models.feedback_memory import FeedbackMemory


class FeedbackManager:
    """í”¼ë“œë°± ê´€ë¦¬ ì„œë¹„ìŠ¤ (RAG ê¸°ë°˜ ì˜¤ë‹µë…¸íŠ¸)"""

    def __init__(self, db: Session, openai_client: AsyncOpenAI):
        self.db = db
        self.openai_client = openai_client

    async def save_feedback(
        self,
        job_category: str,
        competency_name: str,
        ai_score: int,
        ai_reasoning: str,
        human_score: int,
        human_reasoning: str,
        evaluation_id: Optional[int] = None,
        applicant_id: Optional[int] = None,
        use_llm_summary: bool = False  # ğŸ†• V2: ê¸°ë³¸ì ìœ¼ë¡œ LLM ì‚¬ìš© ì•ˆí•¨
    ) -> FeedbackMemory:
        """
        HRì˜ í‰ê°€ ìˆ˜ì •ì„ ì˜¤ë‹µë…¸íŠ¸ë¡œ ì €ì¥ (V2: Simple First)

        Args:
            job_category: ì§ë¬´ ì¹´í…Œê³ ë¦¬ (ì˜ˆ: 'Sales', 'Engineering')
            competency_name: ì—­ëŸ‰ ì´ë¦„ (ì˜ˆ: 'problem_solving')
            ai_score: AIê°€ ë§¤ê¸´ ì ìˆ˜
            ai_reasoning: AIì˜ íŒë‹¨ ê·¼ê±°
            human_score: HRì´ ìˆ˜ì •í•œ ì ìˆ˜
            human_reasoning: HRì˜ ìˆ˜ì • ì‚¬ìœ 
            evaluation_id: ì›ë³¸ í‰ê°€ ID
            applicant_id: ì§€ì›ì ID
            use_llm_summary: LLMìœ¼ë¡œ ìš”ì•½ ìƒì„± (ê¸°ë³¸: False, ë¹ ë¥¸ ì €ì¥)

        Returns:
            ì €ì¥ëœ FeedbackMemory ê°ì²´
        """
        # ğŸ†• V2: LLM ìš”ì•½ì„ ì„ íƒì ìœ¼ë¡œë§Œ ì‚¬ìš©
        if use_llm_summary:
            # ê¸°ì¡´ ë°©ì‹ (LLM 2íšŒ í˜¸ì¶œ)
            mistake_summary = await self._summarize_mistake(
                ai_reasoning=ai_reasoning,
                human_reasoning=human_reasoning,
                ai_score=ai_score,
                human_score=human_score
            )

            correction_guideline = await self._generate_correction_guideline(
                competency_name=competency_name,
                mistake_summary=mistake_summary,
                human_reasoning=human_reasoning
            )
        else:
            # ğŸ†• Simple First: í…œí”Œë¦¿ ì‚¬ìš© (LLM í˜¸ì¶œ 0íšŒ)
            score_diff = human_score - ai_score
            mistake_summary = f"AI ì ìˆ˜ {ai_score}ì ì„ {human_score}ì ìœ¼ë¡œ ì¡°ì • (ì°¨ì´: {score_diff:+d}ì )"

            # HRì˜ ìˆ˜ì • ì‚¬ìœ ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            correction_guideline = human_reasoning

        # ì„ë² ë”© ìƒì„± (HR ìˆ˜ì • ì‚¬ìœ ë¡œ ìƒì„± - ë” ì •í™•)
        embedding_vector = await self._get_embedding(human_reasoning)

        # DB ì €ì¥
        feedback = FeedbackMemory(
            job_category=job_category,
            competency_name=competency_name,
            mistake_summary=mistake_summary,
            ai_score=ai_score,
            ai_reasoning=ai_reasoning,
            human_score=human_score,
            correction_guideline=correction_guideline,
            embedding=embedding_vector,
            evaluation_id=evaluation_id,
            applicant_id=applicant_id
        )

        self.db.add(feedback)
        self.db.commit()
        self.db.refresh(feedback)

        print(f"âœ… Feedback saved: {competency_name} ({job_category}) - {mistake_summary[:50]}...")

        return feedback

    async def get_relevant_feedback(
        self,
        job_category: str,
        competency_name: str,
        current_context: str,
        top_k: int = 3,
        similarity_threshold: float = 0.5,  # ğŸ†• V2: ê¸°ë³¸ê°’ ë‚®ì¶¤
        use_dynamic_threshold: bool = True  # ğŸ†• V2: ë™ì  threshold
    ) -> List[Dict]:
        """
        í˜„ì¬ í‰ê°€ ìƒí™©ê³¼ ìœ ì‚¬í•œ ê³¼ê±° í”¼ë“œë°± ê²€ìƒ‰ (V2: Dynamic Threshold)

        Args:
            job_category: ì§ë¬´ ì¹´í…Œê³ ë¦¬
            competency_name: ì—­ëŸ‰ ì´ë¦„
            current_context: í˜„ì¬ í‰ê°€ ì»¨í…ìŠ¤íŠ¸ (íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ìš”ì•½ ë“±)
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜
            similarity_threshold: ìµœì†Œ ìœ ì‚¬ë„ (0~1)
            use_dynamic_threshold: ë™ì ìœ¼ë¡œ threshold ì¡°ì • (ê¸°ë³¸: True)

        Returns:
            ê´€ë ¨ í”¼ë“œë°± ëª©ë¡ (ìœ ì‚¬ë„ ë†’ì€ ìˆœ)
        """
        # 1. í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ ì„ë² ë”©
        query_vector = await self._get_embedding(current_context)

        # ğŸ†• V2: Dynamic Threshold - ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì ì§„ì ìœ¼ë¡œ ë‚®ì¶¤
        if use_dynamic_threshold:
            return await self._search_with_dynamic_threshold(
                query_vector=query_vector,
                job_category=job_category,
                competency_name=competency_name,
                top_k=top_k,
                initial_threshold=similarity_threshold
            )
        else:
            # ê¸°ì¡´ ë°©ì‹ (ê³ ì • threshold)
            return await self._search_with_threshold(
                query_vector=query_vector,
                job_category=job_category,
                competency_name=competency_name,
                top_k=top_k,
                threshold=similarity_threshold
            )

    async def _search_with_dynamic_threshold(
        self,
        query_vector: List[float],
        job_category: str,
        competency_name: str,
        top_k: int,
        initial_threshold: float
    ) -> List[Dict]:
        """
        ğŸ†• V2: ë™ì  thresholdë¡œ ê²€ìƒ‰ (ê²°ê³¼ ì—†ìœ¼ë©´ ì ì§„ì ìœ¼ë¡œ ì™„í™”)
        """
        thresholds = [initial_threshold, 0.5, 0.3, 0.1, 0.0]

        for threshold in thresholds:
            results = await self._search_with_threshold(
                query_vector=query_vector,
                job_category=job_category,
                competency_name=competency_name,
                top_k=top_k,
                threshold=threshold
            )

            if results:
                if threshold < initial_threshold:
                    print(f"  âš™ï¸  Threshold relaxed to {threshold} (found {len(results)} results)")
                return results

        # ìµœí›„ì˜ ìˆ˜ë‹¨: ë¬´ì¡°ê±´ 1ê°œ ë°˜í™˜
        print(f"  âš ï¸  No results found, returning top 1 without threshold")
        return await self._search_with_threshold(
            query_vector=query_vector,
            job_category=job_category,
            competency_name=competency_name,
            top_k=1,
            threshold=0.0
        )

    async def _search_with_threshold(
        self,
        query_vector: List[float],
        job_category: str,
        competency_name: str,
        top_k: int,
        threshold: float
    ) -> List[Dict]:
        """
        íŠ¹ì • thresholdë¡œ ê²€ìƒ‰
        """
        # pgvector ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰
        sql = text("""
            SELECT
                id,
                job_category,
                competency_name,
                mistake_summary,
                correction_guideline,
                ai_score,
                human_score,
                (1 - (embedding <=> CAST(:query_vector AS vector))) as similarity
            FROM feedback_memory
            WHERE job_category = :job_category
              AND competency_name = :competency_name
              AND (1 - (embedding <=> CAST(:query_vector AS vector))) >= :threshold
            ORDER BY embedding <=> CAST(:query_vector AS vector)
            LIMIT :top_k
        """)

        # query_vectorë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (pgvector í˜•ì‹: '[0.1, 0.2, ...]')
        query_vector_str = "[" + ",".join(map(str, query_vector)) + "]"

        result = self.db.execute(
            sql,
            {
                "query_vector": query_vector_str,
                "job_category": job_category,
                "competency_name": competency_name,
                "threshold": threshold,  # ğŸ†• ìˆ˜ì •: íŒŒë¼ë¯¸í„° ì´ë¦„ ìˆ˜ì •
                "top_k": top_k
            }
        )

        # ê²°ê³¼ ë³€í™˜
        feedbacks = []
        for row in result:
            feedbacks.append({
                "id": row.id,
                "job_category": row.job_category,
                "competency_name": row.competency_name,
                "mistake_summary": row.mistake_summary,
                "correction_guideline": row.correction_guideline,
                "ai_score": row.ai_score,
                "human_score": row.human_score,
                "similarity": float(row.similarity)
            })

        return feedbacks

    async def _summarize_mistake(
        self,
        ai_reasoning: str,
        human_reasoning: str,
        ai_score: int,
        human_score: int
    ) -> str:
        """AIì˜ ì‹¤ìˆ˜ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½"""
        prompt = f"""
AIì™€ HRì˜ í‰ê°€ë¥¼ ë¹„êµí•˜ì—¬, AIê°€ ì–´ë–¤ ë§¥ë½ì„ ë†“ì³ì„œ ì‹¤ìˆ˜í–ˆëŠ”ì§€ **í•œ ë¬¸ì¥**ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.

AI íŒë‹¨ (ì ìˆ˜: {ai_score}ì ):
{ai_reasoning}

HR ìˆ˜ì • (ì ìˆ˜: {human_score}ì ):
{human_reasoning}

ìš”ì•½ (í•œ ë¬¸ì¥, 30ì ì´ë‚´):
"""

        response = await self.openai_client.chat.completions.create(
            model="gpt-4o-mini",  # ë¹ ë¥´ê³  ì €ë ´í•œ ëª¨ë¸
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ í‰ê°€ ì°¨ì´ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.0,
            max_tokens=100
        )

        summary = response.choices[0].message.content.strip()
        return summary

    async def _generate_correction_guideline(
        self,
        competency_name: str,
        mistake_summary: str,
        human_reasoning: str
    ) -> str:
        """êµì • ê°€ì´ë“œë¼ì¸ ìƒì„±"""
        prompt = f"""
ì—­ëŸ‰: {competency_name}
ì‹¤ìˆ˜ ìš”ì•½: {mistake_summary}
HRì˜ ì˜¬ë°”ë¥¸ íŒë‹¨: {human_reasoning}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ìŒ í‰ê°€ ì‹œ AIê°€ ì°¸ê³ í•  ìˆ˜ ìˆëŠ” **êµì • ê°€ì´ë“œë¼ì¸**ì„ ì‘ì„±í•˜ì„¸ìš”.
í˜•ì‹: "~í•  ë•ŒëŠ” ~ë¡œ í•´ì„í•´ì•¼ í•©ë‹ˆë‹¤" (1-2ë¬¸ì¥)
"""

        response = await self.openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ í‰ê°€ ê°€ì´ë“œë¼ì¸ì„ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.0,
            max_tokens=200
        )

        guideline = response.choices[0].message.content.strip()
        return guideline

    async def _get_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ OpenAI Embeddingìœ¼ë¡œ ë³€í™˜"""
        response = await self.openai_client.embeddings.create(
            model="text-embedding-3-small",  # 1536 ì°¨ì›
            input=text
        )

        return response.data[0].embedding
