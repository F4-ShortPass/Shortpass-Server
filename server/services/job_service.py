# server/services/job_service.py
"""
Job ì²˜ë¦¬ ì„œë¹„ìŠ¤ - JD PDF ì—…ë¡œë“œ ë° ë²¡í„°í™”
"""
from sqlalchemy.orm import Session
from typing import Optional, Dict, Any, List
from datetime import datetime
from pydantic import BaseModel, Field

from models.job import Job, JobChunk
try:
    from models.company import Company
except ImportError:
    Company = None  # fallback if company model doesn't exist
from services.s3_service import S3Service
from services.embedding_service import EmbeddingService
from ai.parsers.jd_parser import JDParser
from ai.utils.llm_client import LLMClient


# ==================== Pydantic Models for Persona Generation ====================

class InterviewerPersona(BaseModel):
    """ë©´ì ‘ê´€ í˜ë¥´ì†Œë‚˜ ì •ë³´"""
    company_name: str = Field(description="íšŒì‚¬ëª… (JDì—ì„œ ì¶”ì¶œ)")
    job_title: str = Field(description="ì§ë¬´ëª… (JDì—ì„œ ì¶”ì¶œ)")
    interviewer_identity: str = Field(description="ë©´ì ‘ê´€ ì •ì²´ì„± (ì˜ˆ: 'OOë¶€ë¬¸ 15ë…„ ì°¨ ì‹œë‹ˆì–´ ì±„ìš© ë‹´ë‹¹ì')")
    interviewer_name: str = Field(description="ë©´ì ‘ê´€ ì´ë¦„ (ì˜ˆ: 'ê¹€OO ì±…ì„')")
    interviewer_tone: List[str] = Field(description="ë©´ì ‘ê´€ ë§íˆ¬/ìŠ¤íƒ€ì¼ (3-5ê°œ)", min_length=3)
    initial_questions: List[str] = Field(description="ë©´ì ‘ ì‹œì‘ ì§ˆë¬¸ (3-5ê°œ)", min_length=3, max_length=5)
    system_instruction: str = Field(description="ë©´ì ‘ê´€ ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­")


class JobService:
    """
    ì±„ìš© ê³µê³  ì²˜ë¦¬ ì„œë¹„ìŠ¤

    ì „ì²´ í”Œë¡œìš°:
    1. PDF ì—…ë¡œë“œ â†’ S3 ì €ì¥
    2. PDF ë‹¤ìš´ë¡œë“œ â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    3. í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• 
    4. ì²­í¬ë³„ ì„ë² ë”© ìƒì„± (Bedrock Titan)
    5. DB ì €ì¥ (jobs, job_chunks)
    """

    def __init__(self):
        self.s3_service = S3Service()
        self.embedding_service = EmbeddingService()
        self.jd_parser = JDParser(chunk_size=1000, chunk_overlap=200)
        # self.prompt_builder = ParsingPromptBuilder()  # ì„ì‹œ ë¹„í™œì„±í™”
        self.llm_client = LLMClient()

    async def process_jd_pdf(
        self,
        db: Session,
        pdf_content: bytes,
        file_name: str,
        company_id: int,
        title: str,
        company_url: str = None
    ) -> Job:
        """
        JD PDF ì „ì²´ ì²˜ë¦¬ í”Œë¡œìš°

        Args:
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            pdf_content: PDF íŒŒì¼ ë°”ì´ë„ˆë¦¬
            file_name: íŒŒì¼ëª…
            company_id: íšŒì‚¬ ID
            title: ì±„ìš© ê³µê³  ì œëª©

        Returns:
            Job: ìƒì„±ëœ Job ê°ì²´

        Raises:
            Exception: ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ
        """
        try:
            print(f"\n{'='*60}")
            print(f"Starting JD PDF processing: {file_name}")
            print(f"{'='*60}")

            # 1. S3ì— PDF ì—…ë¡œë“œ
            print("\n[Step 1/5] Uploading PDF to S3...")
            s3_key = self.s3_service.upload_file(
                file_content=pdf_content,
                file_name=file_name,
                folder="jd_pdfs"
            )

            # 2. PDF íŒŒì‹± ë° ì²­í¬ ë¶„í• 
            print("\n[Step 2/5] Parsing PDF and creating chunks...")
            parsed_result = self.jd_parser.parse_and_chunk(
                pdf_content=pdf_content,
                metadata={
                    "company_id": company_id,
                    "s3_key": s3_key,
                    "file_name": file_name
                }
            )

            full_text = parsed_result["full_text"]
            chunks = parsed_result["chunks"]

            print(f"  - Total text length: {len(full_text)} characters")
            print(f"  - Number of chunks: {len(chunks)}")

            # 2-1. JDì—ì„œ íšŒì‚¬ ê°€ì¤‘ì¹˜ ë° ì—­ëŸ‰ ì¶”ì¶œ
            print("\n[Step 2-1/6] Extracting competencies from JD...")
            weights_data = None

            try:
                weights_data = await self._extract_company_weights(full_text)
                print(f"  âœ“ Competencies extracted: {len(weights_data.get('competencies', []))} competencies")
            except Exception as e:
                print(f"  âœ— Failed to extract company weights: {e}")
                print(f"  â†’ Continuing without weight extraction...")
                weights_data = None

            if weights_data and "weights" in weights_data and Company:
                # Company í…Œì´ë¸” ì—…ë°ì´íŠ¸ (Company ëª¨ë¸ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ)
                try:
                    company = db.query(Company).filter(Company.id == company_id).first()
                    if company:
                        company.category_weights = weights_data["weights"]
                        # reasoningë„ ì €ì¥ (ì„ íƒì‚¬í•­)
                        if not company.company_culture_desc and "reasoning" in weights_data:
                            company.company_culture_desc = str(weights_data.get("reasoning", {}))
                        db.flush()
                        print(f"  âœ“ Company weights updated: {weights_data['weights']}")
                    else:
                        print(f"  âš  Company {company_id} not found, skipping weight update")
                except Exception as e:
                    print(f"  âš  Failed to update company weights: {e}")
            else:
                print("  âš  Skipping company weight update (no Company model or no weight data)")

            # 2-2. ğŸ†• ë™ì  Persona ìƒì„±
            print("\n[Step 2-2/7] Generating interviewer persona from JD...")
            persona_data = None

            try:
                competencies = weights_data.get("competencies", []) if weights_data else []
                persona_data = await self._generate_persona_from_jd(full_text, competencies)
                if persona_data:
                    print(f"  âœ“ Persona generated: {persona_data.company_name} - {persona_data.job_title}")
                else:
                    print(f"  âš  Persona generation returned None")
            except Exception as e:
                print(f"  âœ— Failed to generate persona: {e}")
                print(f"  â†’ Continuing without persona data...")
                persona_data = None

            # 3. Job ìƒì„±
            print("\n[Step 3/7] Creating Job record...")
            job = Job(
                company_id=company_id,
                title=title,
                description=full_text,
                company_url=company_url  # ê¸°ì—… URL ì €ì¥ (í–¥í›„ íŒŒì‹± ì˜ˆì •)
            )

            # ì—­ëŸ‰ ì •ë³´ ì €ì¥ (ì¶”ì¶œ ì„±ê³µ ì‹œ)
            if weights_data and "competencies" in weights_data:
                # Job í…Œì´ë¸”ì— ì—­ëŸ‰ ì •ë³´ ì €ì¥
                job.dynamic_evaluation_criteria = weights_data.get("competencies", [])
                job.competency_weights = weights_data.get("weights", {})
                job.weights_reasoning = weights_data.get("reasoning", "")
                print(f"  âœ“ Stored {len(weights_data['competencies'])} competencies in Job")

            # ğŸ†• Persona ì •ë³´ ì €ì¥ (ìƒì„± ì„±ê³µ ì‹œ)
            if persona_data:
                job.company_name = persona_data.company_name
                job.job_title = persona_data.job_title
                job.interviewer_identity = persona_data.interviewer_identity
                job.interviewer_name = persona_data.interviewer_name
                job.interviewer_tone = persona_data.interviewer_tone
                job.initial_questions = persona_data.initial_questions
                job.system_instruction = persona_data.system_instruction
                print(f"  âœ“ Stored persona data in Job")

            db.add(job)
            db.flush()  # ID ìƒì„±ì„ ìœ„í•´ flush
            print(f"  - Job created with ID: {job.id}")

            # 4. ì²­í¬ë³„ ì„ë² ë”© ìƒì„±
            print("\n[Step 4/5] Generating embeddings for chunks...")
            print(f"  - Total chunks to embed: {len(chunks)}")
            chunk_texts = [chunk["chunk_text"] for chunk in chunks]

            try:
                embeddings = self.embedding_service.generate_embeddings_batch(
                    texts=chunk_texts,
                    batch_size=5  # API ì œí•œ ê³ ë ¤
                )
            except Exception as e:
                print(f"  âœ— Embedding generation failed: {e}")
                raise Exception(f"Failed to generate embeddings: {str(e)}")

            # 5. JobChunk ì €ì¥
            print("\n[Step 5/5] Saving chunks to database...")
            created_chunks = []

            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                if embedding is None:
                    print(f"  âš  Skipping chunk {i} (embedding failed)")
                    continue

                job_chunk = JobChunk(
                    job_id=job.id,
                    chunk_text=chunk["chunk_text"],
                    embedding=embedding,
                    chunk_index=chunk["chunk_index"]
                )
                db.add(job_chunk)
                created_chunks.append(job_chunk)

            # ì»¤ë°‹
            db.commit()
            db.refresh(job)

            print(f"\n{'='*60}")
            print(f"âœ“ JD Processing completed successfully!")
            print(f"  - Job ID: {job.id}")
            print(f"  - Chunks saved: {len(created_chunks)}")
            print(f"  - S3 Key: {s3_key}")
            print(f"{'='*60}\n")

            return job

        except Exception as e:
            db.rollback()
            print(f"\nâœ— JD Processing failed: {e}")
            raise Exception(f"Failed to process JD PDF: {str(e)}")

    def get_job_with_chunks(
        self,
        db: Session,
        job_id: int
    ) -> Optional[Dict[str, Any]]:
        """
        Jobê³¼ ëª¨ë“  ì²­í¬ ì¡°íšŒ

        Args:
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            job_id: Job ID

        Returns:
            Dict: Job ì •ë³´ + ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        job = db.query(Job).filter(Job.id == job_id).first()

        if not job:
            return None

        chunks = db.query(JobChunk).filter(
            JobChunk.job_id == job_id
        ).order_by(JobChunk.chunk_index).all()

        return {
            "job_id": job.id,
            "company_id": job.company_id,
            "title": job.title,
            "description": job.description,
            "created_at": job.created_at,
            "chunks": [
                {
                    "chunk_id": chunk.id,
                    "chunk_text": chunk.chunk_text,
                    "chunk_index": chunk.chunk_index,
                    "has_embedding": chunk.embedding is not None
                }
                for chunk in chunks
            ],
            "total_chunks": len(chunks)
        }

    def search_similar_chunks(
        self,
        db: Session,
        query_text: str,
        top_k: int = 5,
        job_id: Optional[int] = None
    ) -> List[Dict[str, Any]]:
        """
        ìœ ì‚¬í•œ ì²­í¬ ê²€ìƒ‰ (ë²¡í„° ìœ ì‚¬ë„)

        Args:
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            query_text: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜
            job_id: íŠ¹ì • Jobìœ¼ë¡œ ì œí•œ (ì„ íƒ)

        Returns:
            List[Dict]: ìœ ì‚¬í•œ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        from pgvector.sqlalchemy import cosine_distance

        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        print(f"Generating embedding for query: {query_text[:100]}...")
        query_embedding = self.embedding_service.generate_embedding(query_text)

        # ë²¡í„° ê²€ìƒ‰
        query = db.query(
            JobChunk.id,
            JobChunk.job_id,
            JobChunk.chunk_text,
            JobChunk.chunk_index,
            cosine_distance(JobChunk.embedding, query_embedding).label("distance")
        )

        if job_id:
            query = query.filter(JobChunk.job_id == job_id)

        results = query.order_by("distance").limit(top_k).all()

        return [
            {
                "chunk_id": r.id,
                "job_id": r.job_id,
                "chunk_text": r.chunk_text,
                "chunk_index": r.chunk_index,
                "similarity": 1 - r.distance  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
            }
            for r in results
        ]

    def delete_job(
        self,
        db: Session,
        job_id: int
    ) -> bool:
        """
        Job ë° ê´€ë ¨ ì²­í¬ ì‚­ì œ

        Args:
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            job_id: Job ID

        Returns:
            bool: ì‚­ì œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            job = db.query(Job).filter(Job.id == job_id).first()

            if not job:
                return False

            # Job ì‚­ì œ (CASCADEë¡œ ì²­í¬ë„ ìë™ ì‚­ì œ)
            db.delete(job)
            db.commit()

            print(f"âœ“ Job {job_id} deleted successfully")
            return True

        except Exception as e:
            db.rollback()
            print(f"âœ— Failed to delete job {job_id}: {e}")
            return False

    async def _extract_company_weights(self, jd_text: str) -> Optional[Dict[str, Any]]:
        """
        JD í…ìŠ¤íŠ¸ì—ì„œ íšŒì‚¬ì˜ í•µì‹¬ ì—­ëŸ‰ ê°€ì¤‘ì¹˜ ì¶”ì¶œ

        ë©€í‹°ì—ì´ì „íŠ¸ í‰ê°€ ì‹œìŠ¤í…œê³¼ ì—°ë™ì„ ìœ„í•´ 5ê°œ ê³ ì • ì»¨ì„¤íŒ… ì—­ëŸ‰ìœ¼ë¡œ ë§¤í•‘

        Args:
            jd_text: JD ì „ì²´ í…ìŠ¤íŠ¸

        Returns:
            Dict: {
                "weights": {...},
                "reasoning": {...},
                "competencies": [5ê°œ ê³ ì • ì—­ëŸ‰]
            }
        """
        print(f"[_extract_company_weights] Starting competency extraction...")
        try:
            # 5ê°œ ê³ ì • ì»¨ì„¤íŒ… ì—­ëŸ‰ (ë©€í‹°ì—ì´ì „íŠ¸ í‰ê°€ì™€ ë™ì¼)
            FIXED_COMPETENCIES = [
                "Strategic Planning & Analysis (ì „ëµ ê¸°íš ë° ë¶„ì„ë ¥)",
                "Stakeholder Management (ì´í•´ê´€ê³„ì ê´€ë¦¬)",
                "Project & Timeline Management (í”„ë¡œì íŠ¸ ì‹¤í–‰ ê´€ë¦¬)",
                "Business Insight & Market Research (ì‹œì¥ ì¡°ì‚¬ ë° ì¸ì‚¬ì´íŠ¸)",
                "Data Management & Reporting (ë°ì´í„° ê´€ë¦¬ ë° ë³´ê³ )"
            ]

            prompt = f"""
ë‹¹ì‹ ì€ ì±„ìš© ê³µê³ (JD)ë¥¼ ë¶„ì„í•˜ì—¬ íšŒì‚¬ê°€ ìš”êµ¬í•˜ëŠ” ì—­ëŸ‰ì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ JDë¥¼ ë¶„ì„í•˜ì—¬, ì•„ë˜ **5ê°œ ì»¨ì„¤íŒ… ì§ë¬´ ì—­ëŸ‰** ê°ê°ì— ëŒ€í•´ ì´ JDê°€ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•˜ê²Œ ìš”êµ¬í•˜ëŠ”ì§€ ì ìˆ˜(0-100)ë¥¼ ë§¤ê¸°ì„¸ìš”.

**ë¶„ì„í•  5ê°œ ì—­ëŸ‰:**
1. Strategic Planning & Analysis (ì „ëµ ê¸°íš ë° ë¶„ì„ë ¥)
   - ì „ëµ ìˆ˜ë¦½, ë¬¸ì œ ë¶„ì„, ì˜ì‚¬ê²°ì •, ë…¼ë¦¬ì  ì‚¬ê³ 
2. Stakeholder Management (ì´í•´ê´€ê³„ì ê´€ë¦¬)
   - ì»¤ë®¤ë‹ˆì¼€ì´ì…˜, í˜‘ì—…, ì„¤ë“ë ¥, ê´€ê³„ êµ¬ì¶•
3. Project & Timeline Management (í”„ë¡œì íŠ¸ ì‹¤í–‰ ê´€ë¦¬)
   - ì¼ì • ê´€ë¦¬, ì—…ë¬´ ì¡°ìœ¨, ì‹¤í–‰ë ¥, ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
4. Business Insight & Market Research (ì‹œì¥ ì¡°ì‚¬ ë° ì¸ì‚¬ì´íŠ¸)
   - ì‹œì¥ ë¶„ì„, íŠ¸ë Œë“œ íŒŒì•…, ê³ ê° ì´í•´, ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
5. Data Management & Reporting (ë°ì´í„° ê´€ë¦¬ ë° ë³´ê³ )
   - ë°ì´í„° ë¶„ì„, ë³´ê³ ì„œ ì‘ì„±, ì§€í‘œ ê´€ë¦¬, ê²°ê³¼ ì •ë¦¬

**JD:**
{jd_text[:3000]}

**ì‘ë‹µ í˜•ì‹ (JSON):**
{{
  "competencies": [
    {{
      "name": "Strategic Planning & Analysis (ì „ëµ ê¸°íš ë° ë¶„ì„ë ¥)",
      "category": "technical",
      "score": 85,
      "description": "ì´ JDì—ì„œ ì´ ì—­ëŸ‰ì´ ì¤‘ìš”í•œ ì´ìœ "
    }},
    {{
      "name": "Stakeholder Management (ì´í•´ê´€ê³„ì ê´€ë¦¬)",
      "category": "cultural",
      "score": 75,
      "description": "..."
    }},
    ... (5ê°œ ëª¨ë‘)
  ],
  "category_weights": {{
    "technical": 0.35,
    "cultural": 0.30,
    "experience": 0.20,
    "leadership": 0.15
  }},
  "reasoning": "JD ì „ì²´ ë¶„ì„ ìš”ì•½"
}}

**ì¤‘ìš”:**
- ë°˜ë“œì‹œ ìœ„ 5ê°œ ì—­ëŸ‰ ëª¨ë‘ì— ëŒ€í•´ ì ìˆ˜ë¥¼ ë§¤ê¸°ì„¸ìš”
- ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”
- category_weightsì˜ í•©ì€ 1.0ì´ì–´ì•¼ í•©ë‹ˆë‹¤
"""

            print(f"[_extract_company_weights] Calling LLM with prompt length: {len(prompt)} chars")
            response = await self.llm_client.generate(
                prompt=prompt,
                max_tokens=2000,
                temperature=0.3
            )
            print(f"[_extract_company_weights] âœ“ LLM response received, length: {len(str(response))} chars")

            # JSON íŒŒì‹±
            import json
            import re

            print(f"[_extract_company_weights] Parsing LLM response...")

            # responseê°€ dictì¸ ê²½ìš° (generate ë©”ì„œë“œê°€ dictë¥¼ ë°˜í™˜í•  ë•Œ)
            if isinstance(response, dict):
                if 'text' in response:
                    response_text = response['text']
                else:
                    print(f"[_extract_company_weights] âš  Unexpected response format: {response}")
                    response_text = str(response)
            else:
                response_text = str(response)

            # JSON ì¶”ì¶œ (```json ... ``` í˜•íƒœì¸ ê²½ìš° ì²˜ë¦¬)
            json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # JSON ë¸”ë¡ì´ ì—†ìœ¼ë©´ ì „ì²´ë¥¼ JSONìœ¼ë¡œ íŒŒì‹± ì‹œë„
                json_str = response_text

            try:
                result = json.loads(json_str)
                print(f"[_extract_company_weights] âœ“ JSON parsed successfully")
            except json.JSONDecodeError as e:
                print(f"[_extract_company_weights] âœ— JSON parsing failed: {e}")
                print(f"  Response preview: {response_text[:500]}...")
                raise

            # 5ê°œ ì—­ëŸ‰ì´ ëª¨ë‘ ìˆëŠ”ì§€ ê²€ì¦
            competencies = result.get("competencies", [])
            if len(competencies) != 5:
                print(f"[_extract_company_weights] âš  Warning: Expected 5 competencies, got {len(competencies)}")
                # ë¶€ì¡±í•œ ì—­ëŸ‰ì€ ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
                existing_names = {c.get("name", "") for c in competencies}
                for fixed_comp in FIXED_COMPETENCIES:
                    if fixed_comp not in existing_names:
                        competencies.append({
                            "name": fixed_comp,
                            "category": "technical",
                            "score": 50,
                            "description": "ë¶„ì„ë˜ì§€ ì•ŠìŒ"
                        })

            final_result = {
                "weights": result.get("category_weights", {}),
                "competencies": competencies[:5],  # ìµœëŒ€ 5ê°œë§Œ
                "reasoning": result.get("reasoning", "")
            }
            print(f"[_extract_company_weights] âœ“ Extraction completed: {len(final_result['competencies'])} competencies")
            return final_result

        except Exception as e:
            print(f"[_extract_company_weights] âœ— Failed to extract company weights: {e}")
            print(f"   Error type: {type(e).__name__}")
            import traceback
            print(f"   Traceback: {traceback.format_exc()}")
            return None

    async def _generate_persona_from_jd(
        self,
        jd_text: str,
        competencies: Optional[List[Dict]] = None
    ) -> Optional[InterviewerPersona]:
        """
        JD í…ìŠ¤íŠ¸ì—ì„œ ë©´ì ‘ê´€ í˜ë¥´ì†Œë‚˜ë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±

        Args:
            jd_text: JD ì „ì²´ í…ìŠ¤íŠ¸
            competencies: ì¶”ì¶œëœ ì§ë¬´ ì—­ëŸ‰ ë¦¬ìŠ¤íŠ¸ (ì„ íƒ)

        Returns:
            InterviewerPersona: ë©´ì ‘ê´€ í˜ë¥´ì†Œë‚˜ ì •ë³´ ë˜ëŠ” None
        """
        print(f"[_generate_persona_from_jd] Starting persona generation...")

        try:
            # ì—­ëŸ‰ ì •ë³´ í¬ë§·íŒ…
            competencies_text = ""
            if competencies:
                competencies_text = "\n".join([
                    f"- {comp.get('name', 'Unknown')}: {comp.get('description', '')}"
                    for comp in competencies[:5]
                ])
            else:
                competencies_text = "(ì—­ëŸ‰ ì •ë³´ ì—†ìŒ)"

            prompt = f"""
ë‹¹ì‹ ì€ ì±„ìš© ê³µê³ (JD)ë¥¼ ë¶„ì„í•˜ì—¬ AI ë©´ì ‘ê´€ í˜ë¥´ì†Œë‚˜ë¥¼ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ JDë¥¼ ë¶„ì„í•˜ì—¬ ë©´ì ‘ê´€ í˜ë¥´ì†Œë‚˜ë¥¼ ìƒì„±í•˜ì„¸ìš”.

**JD:**
{jd_text[:3000]}

**ì¶”ì¶œëœ í•µì‹¬ ì—­ëŸ‰:**
{competencies_text}

**ìš”êµ¬ì‚¬í•­:**
1. **company_name**: JDì—ì„œ íšŒì‚¬ëª…ì„ ì¶”ì¶œí•˜ì„¸ìš”. ì—†ìœ¼ë©´ "ë¯¸ìƒ" ë˜ëŠ” "ê¸°ì—…"ìœ¼ë¡œ í‘œê¸°
2. **job_title**: êµ¬ì²´ì ì¸ ì§ë¬´ëª… (ì˜ˆ: "ë°±ì—”ë“œ ì—”ì§€ë‹ˆì–´", "ìƒí’ˆê¸°íš(MD)")
3. **interviewer_identity**: ë©´ì ‘ê´€ì˜ ì •ì²´ì„± (ì˜ˆ: "OOë¶€ë¬¸ 15ë…„ ì°¨ ì‹œë‹ˆì–´ ì±„ìš© ë‹´ë‹¹ì", "ê¸°ìˆ íŒ€ ë¦¬ë“œ")
4. **interviewer_name**: ê°€ìƒì˜ ë©´ì ‘ê´€ ì´ë¦„ (ì˜ˆ: "ê¹€OO ì±…ì„", "ì´OO ë§¤ë‹ˆì €")
5. **interviewer_tone**: ë©´ì ‘ê´€ì˜ ë§íˆ¬ì™€ ìŠ¤íƒ€ì¼ (3-5ê°œ í•­ëª©)
   - ì˜ˆ: ["ì „ë¬¸ì ì´ê³  ì •ì¤‘í•¨", "ë…¼ë¦¬ì  ê·¼ê±°ë¥¼ ì¤‘ì‹œí•¨", "êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ ì¬ì§ˆë¬¸í•¨"]
6. **initial_questions**: ë©´ì ‘ ì‹œì‘ ì§ˆë¬¸ 3-5ê°œ
   - íšŒì‚¬ì™€ ì§ë¬´ì— íŠ¹í™”ëœ ì§ˆë¬¸
   - ì§€ì›ìì˜ ê²½í—˜ê³¼ ë™ê¸°ë¥¼ íŒŒì•…í•˜ëŠ” ì§ˆë¬¸
   - í•´ë‹¹ ì§ë¬´ì˜ í•µì‹¬ ì—­ëŸ‰ì„ í‰ê°€í•˜ëŠ” ì§ˆë¬¸
7. **system_instruction**: ë©´ì ‘ê´€ ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­ (1-2ë¬¸ì¥)
   - ì˜ˆ: "ë‹¹ì‹ ì€ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì§€ì›ìì˜ ê²½í—˜ì´ ìš°ë¦¬ íšŒì‚¬ì˜ JDì™€ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€ì¦í•˜ì„¸ìš”."

**ì¤‘ìš”:**
- íšŒì‚¬ì™€ ì§ë¬´ì— ë§ëŠ” êµ¬ì²´ì ì´ê³  í˜„ì‹¤ì ì¸ í˜ë¥´ì†Œë‚˜ë¥¼ ìƒì„±í•˜ì„¸ìš”
- ë©´ì ‘ ì§ˆë¬¸ì€ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì´ ì•„ë‹Œ, í•´ë‹¹ ì§ë¬´ì™€ íšŒì‚¬ì— íŠ¹í™”ëœ ì§ˆë¬¸ì´ì–´ì•¼ í•©ë‹ˆë‹¤
- í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”
"""

            print(f"[_generate_persona_from_jd] Calling OpenAI with structured output...")

            # OpenAI API ì§ì ‘ í˜¸ì¶œ (Pydantic Structured Output)
            from openai import AsyncOpenAI
            from core.config import OPENAI_API_KEY

            client = AsyncOpenAI(api_key=OPENAI_API_KEY)

            response = await client.beta.chat.completions.parse(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert in creating interviewer personas from job descriptions."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.4,
                max_tokens=2000,
                response_format=InterviewerPersona
            )

            # Pydantic ëª¨ë¸ë¡œ ìë™ íŒŒì‹±ë¨
            persona = response.choices[0].message.parsed

            print(f"[_generate_persona_from_jd] âœ“ Persona generated successfully")
            print(f"  - Company: {persona.company_name}")
            print(f"  - Job Title: {persona.job_title}")
            print(f"  - Initial Questions: {len(persona.initial_questions)} questions")

            return persona

        except Exception as e:
            print(f"[_generate_persona_from_jd] âœ— Failed to generate persona: {e}")
            print(f"   Error type: {type(e).__name__}")
            import traceback
            print(f"   Traceback: {traceback.format_exc()}")
            return None
