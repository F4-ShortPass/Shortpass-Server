# server/api/job.py
"""
Job ê´€ë ¨ API ì—”ë“œí¬ì¸íŠ¸
"""
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, Form
from sqlalchemy.orm import Session
from typing import Optional, List
import logging

from db.database import get_db
from services.job_service import JobService
from schemas.evaluation import ApplicantListResponse
from pydantic import BaseModel


router = APIRouter(prefix="/jobs", tags=["jobs"])
logger = logging.getLogger("uvicorn")

@router.get("/{job_id}/applicants", response_model=ApplicantListResponse)
async def get_applicants_for_job(
    job_id: int,
    db: Session = Depends(get_db)
):
    """
    íŠ¹ì • Jobì— ëŒ€í•œ ì§€ì›ì ëª©ë¡ ë° í‰ê°€ ìš”ì•½ ì¡°íšŒ

    Args:
        job_id: Job ID

    Returns:
        ApplicantListResponse: ì§€ì›ì ëª©ë¡ ë° ìš”ì•½ ì •ë³´
    """
    logger.info(f"Getting applicants for job ID: {job_id}")

    from models.job import Job
    from models.interview import Applicant, Company
    from models.evaluation import Evaluation
    from sqlalchemy import func

    # 1. Job ì¡°íšŒ
    job = db.query(Job).filter(Job.id == job_id).first()
    if not job:
        raise HTTPException(status_code=404, detail=f"Job {job_id} not found")

    # 2. Company ì¡°íšŒ
    company = db.query(Company).filter(Company.id == job.company_id).first()
    company_name = company.name if company else "Unknown Company"

    # 3. í•´ë‹¹ Jobì— ëŒ€í•œ ëª¨ë“  í‰ê°€ ì¡°íšŒ
    evaluations = db.query(Evaluation).filter(
        Evaluation.job_id == job_id
    ).order_by(Evaluation.match_score.desc()).all()

    # 4. í†µê³„ ê³„ì‚°
    total_applicants = len(evaluations)
    completed_evaluations = len([e for e in evaluations if e.evaluation_status == "completed"])

    # í‰ê·  ì ìˆ˜ ê³„ì‚°
    if completed_evaluations > 0:
        scores = [e.match_score for e in evaluations if e.evaluation_status == "completed"]
        average_score = sum(scores) / len(scores)
    else:
        average_score = 0.0

    # 5. ì§€ì›ì ëª©ë¡ êµ¬ì„±
    applicants_list = []

    for rank, evaluation in enumerate(evaluations, start=1):
        # Applicant ì¡°íšŒ
        applicant = db.query(Applicant).filter(Applicant.id == evaluation.applicant_id).first()
        if not applicant:
            continue

        # ê°•ì /ì•½ì  ì¶”ì¶œ (job_aggregationì—ì„œ)
        strengths = []
        weaknesses = []
        competency_scores = []

        if evaluation.job_aggregation:
            for comp_name, comp_data in evaluation.job_aggregation.items():
                if isinstance(comp_data, dict):
                    score = comp_data.get("score", 0)
                else:
                    score = comp_data

                competency_scores.append({
                    "name": comp_name,
                    "score": score
                })

                # ê°•ì /ì•½ì  íŒë‹¨ (80 ì´ìƒì€ ê°•ì , 60 ì´í•˜ëŠ” ì•½ì )
                if score >= 80:
                    strengths.append(comp_name)
                elif score <= 60:
                    weaknesses.append(comp_name)

        # AI ìš”ì•½ ì¶”ì¶œ
        ai_summary = ""
        if evaluation.fit_analysis and isinstance(evaluation.fit_analysis, dict):
            ai_summary = evaluation.fit_analysis.get("summary", "") or \
                        evaluation.fit_analysis.get("overall_assessment", "")

        # ìƒíƒœ ê²°ì •
        score = evaluation.match_score
        if score >= 85:
            status = "ğŸŸ¢ ì¶”ì²œ"
        elif score >= 70:
            status = "ğŸŸ¡ ë³´ë¥˜"
        elif score >= 60:
            status = "ğŸŸ  ê²€í†  í•„ìš”"
        else:
            status = "ğŸ”´ ë¯¸í¡"

        applicants_list.append({
            "applicant_id": f"CAND_{applicant.id:03d}",
            "job_id": f"JOB_{job_id:03d}",
            "rank": rank,
            "applicant_name": applicant.name,
            "track": job.title,
            "total_score": round(score),
            "strengths": ", ".join(strengths) if strengths else "N/A",
            "weaknesses": ", ".join(weaknesses) if weaknesses else "N/A",
            "ai_summary_comment": ai_summary or "í‰ê°€ ì§„í–‰ ì¤‘",
            "status": status,
            "competency_scores": competency_scores[:5]  # ìƒìœ„ 5ê°œë§Œ
        })

    # 6. ì‘ë‹µ êµ¬ì„±
    return {
        "company_name": company_name,
        "job_title": job.title,
        "total_applicants": total_applicants,
        "completed_evaluations": completed_evaluations,
        "average_score": round(average_score, 1),
        "applicants": applicants_list
    }





router = APIRouter(prefix="/jobs", tags=["jobs"])
logger = logging.getLogger("uvicorn")


# Response Models
class JobResponse(BaseModel):
    job_id: int
    company_id: int
    title: str
    created_at: str
    total_chunks: int

    class Config:
        from_attributes = True


class ChunkResponse(BaseModel):
    chunk_id: int
    chunk_text: str
    chunk_index: int
    has_embedding: bool


class JobDetailResponse(BaseModel):
    job_id: int
    company_id: int
    title: str
    description: str
    created_at: str
    chunks: List[ChunkResponse]
    total_chunks: int


class SearchResult(BaseModel):
    chunk_id: int
    job_id: int
    chunk_text: str
    chunk_index: int
    similarity: float


# Endpoints
@router.post("/upload", response_model=JobResponse)
async def upload_jd_pdf(
    pdf_file: UploadFile = File(..., description="JD PDF íŒŒì¼"),
    company_id: int = Form(..., description="íšŒì‚¬ ID"),
    title: str = Form(..., description="ì±„ìš© ê³µê³  ì œëª©"),
    db: Session = Depends(get_db)
):
    """
    JD PDF ì—…ë¡œë“œ ë° ì²˜ë¦¬

    ì „ì²´ í”Œë¡œìš°:
    1. PDF íŒŒì¼ ì—…ë¡œë“œ ë°›ê¸°
    2. S3ì— ì €ì¥
    3. PDF íŒŒì‹± ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
    4. ì²­í¬ ë¶„í• 
    5. Bedrock Titanìœ¼ë¡œ ì„ë² ë”© ìƒì„±
    6. DBì— ì €ì¥ (jobs, job_chunks)

    Args:
        pdf_file: PDF íŒŒì¼
        company_id: íšŒì‚¬ ID
        title: ì±„ìš© ê³µê³  ì œëª©

    Returns:
        JobResponse: ìƒì„±ëœ Job ì •ë³´
    """
    logger.info(f"Uploading JD for company ID: {company_id}, title: {title}")
    # PDF íŒŒì¼ ê²€ì¦
    if not pdf_file.filename.endswith('.pdf'):
        raise HTTPException(
            status_code=400,
            detail="Only PDF files are allowed"
        )

    # íŒŒì¼ í¬ê¸° ì œí•œ (ì˜ˆ: 10MB)
    max_size = 10 * 1024 * 1024  # 10MB
    pdf_content = await pdf_file.read()

    if len(pdf_content) > max_size:
        raise HTTPException(
            status_code=400,
            detail=f"File size exceeds maximum limit of {max_size / (1024*1024)}MB"
        )

    # Job ì„œë¹„ìŠ¤ë¡œ ì²˜ë¦¬
    try:
        job_service = JobService()
        job = await job_service.process_jd_pdf(
            db=db,
            pdf_content=pdf_content,
            file_name=pdf_file.filename,
            company_id=company_id,
            title=title
        )

        # ì²­í¬ ê°œìˆ˜ ì¡°íšŒ
        chunk_count = len(job.chunks)

        return JobResponse(
            job_id=job.id,
            company_id=job.company_id,
            title=job.title,
            created_at=job.created_at.isoformat(),
            total_chunks=chunk_count
        )

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to process JD PDF: {str(e)}"
        )


@router.get("/{job_id}", response_model=JobDetailResponse)
async def get_job(
    job_id: int,
    db: Session = Depends(get_db)
):
    """
    Job ìƒì„¸ ì •ë³´ ì¡°íšŒ (ì²­í¬ í¬í•¨)

    Args:
        job_id: Job ID

    Returns:
        JobDetailResponse: Job ìƒì„¸ ì •ë³´
    """
    logger.info(f"Getting job with ID: {job_id}")
    job_service = JobService()
    job_data = job_service.get_job_with_chunks(db, job_id)

    if not job_data:
        raise HTTPException(status_code=404, detail="Job not found")

    return JobDetailResponse(
        job_id=job_data["job_id"],
        company_id=job_data["company_id"],
        title=job_data["title"],
        description=job_data["description"],
        created_at=job_data["created_at"].isoformat(),
        chunks=[
            ChunkResponse(
                chunk_id=chunk["chunk_id"],
                chunk_text=chunk["chunk_text"],
                chunk_index=chunk["chunk_index"],
                has_embedding=chunk["has_embedding"]
            )
            for chunk in job_data["chunks"]
        ],
        total_chunks=job_data["total_chunks"]
    )


@router.post("/search", response_model=List[SearchResult])
async def search_similar_chunks(
    query: str = Form(..., description="ê²€ìƒ‰ ì¿¼ë¦¬"),
    top_k: int = Form(5, description="ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜"),
    job_id: Optional[int] = Form(None, description="íŠ¹ì • Jobìœ¼ë¡œ ì œí•œ"),
    db: Session = Depends(get_db)
):
    """
    ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ ì²­í¬ ê²€ìƒ‰

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬ í…ìŠ¤íŠ¸
        top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜
        job_id: íŠ¹ì • Jobìœ¼ë¡œ ì œí•œ (ì„ íƒ)

    Returns:
        List[SearchResult]: ìœ ì‚¬í•œ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    logger.info(f"Searching for similar chunks with query: {query}")
    try:
        job_service = JobService()
        results = job_service.search_similar_chunks(
            db=db,
            query_text=query,
            top_k=top_k,
            job_id=job_id
        )

        return [
            SearchResult(
                chunk_id=r["chunk_id"],
                job_id=r["job_id"],
                chunk_text=r["chunk_text"],
                chunk_index=r["chunk_index"],
                similarity=r["similarity"]
            )
            for r in results
        ]

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Search failed: {str(e)}"
        )


@router.delete("/{job_id}")
async def delete_job(
    job_id: int,
    db: Session = Depends(get_db)
):
    """
    Job ì‚­ì œ (ì²­í¬ë„ í•¨ê»˜ ì‚­ì œë¨)

    Args:
        job_id: Job ID

    Returns:
        Dict: ì‚­ì œ ê²°ê³¼
    """
    logger.info(f"Deleting job with ID: {job_id}")
    job_service = JobService()
    success = job_service.delete_job(db, job_id)

    if not success:
        raise HTTPException(status_code=404, detail="Job not found")

    return {"message": f"Job {job_id} deleted successfully"}


class CompetencyAnalysisResponse(BaseModel):
    """í•µì‹¬ ì—­ëŸ‰ ë¶„ì„ ê²°ê³¼"""
    competencies: List[dict]
    category_weights: dict
    reasoning: str

    class Config:
        from_attributes = True


@router.post("/analyze-competencies", response_model=CompetencyAnalysisResponse)
async def analyze_jd_competencies(
    pdf_file: UploadFile = File(..., description="JD PDF íŒŒì¼"),
    company_id: int = Form(..., description="íšŒì‚¬ ID"),
    company_url: Optional[str] = Form(None, description="íšŒì‚¬ URL (í•µì‹¬ ê°€ì¹˜ ë¶„ì„ìš©)"),
    db: Session = Depends(get_db)
):
    """
    JD PDFì—ì„œ í•µì‹¬ ì—­ëŸ‰ ë¶„ì„

    ì „ì²´ í”Œë¡œìš°:
    1. PDF íŒŒì¼ ì½ê¸° ë° íŒŒì‹±
    2. JD í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ì—­ëŸ‰ ì¶”ì¶œ
    3. (ì„ íƒ) company_urlì´ ìˆìœ¼ë©´ RAGë¡œ íšŒì‚¬ ê°€ì¹˜ ê²€ìƒ‰
    4. ì—­ëŸ‰ ë¶„ì„ ê²°ê³¼ ë°˜í™˜

    Args:
        pdf_file: JD PDF íŒŒì¼
        company_id: íšŒì‚¬ ID
        company_url: íšŒì‚¬ ì†Œê°œ URL (ì„ íƒ)

    Returns:
        CompetencyAnalysisResponse: ì—­ëŸ‰ ë¶„ì„ ê²°ê³¼
    """
    logger.info(f"Analyzing JD competencies for company ID: {company_id}")
    # PDF íŒŒì¼ ê²€ì¦
    if not pdf_file.filename.endswith('.pdf'):
        raise HTTPException(
            status_code=400,
            detail="Only PDF files are allowed"
        )

    # íŒŒì¼ í¬ê¸° ì œí•œ (10MB)
    max_size = 10 * 1024 * 1024
    pdf_content = await pdf_file.read()

    if len(pdf_content) > max_size:
        raise HTTPException(
            status_code=400,
            detail=f"File size exceeds maximum limit of {max_size / (1024*1024)}MB"
        )

    try:
        print(f"\n{'='*60}")
        print(f"[analyze_jd_competencies] Starting JD competency analysis")
        print(f"{'='*60}")
        print(f"  - Company ID: {company_id}")
        print(f"  - Company URL: {company_url or 'Not provided'}")
        print(f"  - PDF file: {pdf_file.filename}")

        job_service = JobService()

        # 1. PDF íŒŒì‹±
        print("\n[Step 1/3] Parsing PDF...")
        from ai.parsers.jd_parser import JDParser
        jd_parser = JDParser()

        try:
            parsed_result = jd_parser.parse_and_chunk(pdf_content=pdf_content)
            full_text = parsed_result["full_text"]
            print(f"  âœ“ PDF parsed: {len(full_text)} characters")
        except Exception as e:
            print(f"  âœ— PDF parsing failed: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to parse PDF: {str(e)}"
            )

        # 2. í•µì‹¬ ì—­ëŸ‰ ì¶”ì¶œ
        print(f"\n[Step 2/3] Extracting competencies from JD...")

        try:
            analysis_result = await job_service._extract_company_weights(full_text)
        except Exception as e:
            print(f"  âœ— Competency extraction failed: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to extract competencies: {str(e)}"
            )

        if not analysis_result:
            print(f"  âœ— No analysis result returned")
            raise HTTPException(
                status_code=500,
                detail="Failed to analyze competencies - no result returned"
            )

        print(f"  âœ“ Competencies extracted: {len(analysis_result.get('competencies', []))} competencies")

        # 3. (ì„ íƒ) Company URLì´ ìˆìœ¼ë©´ DBì— ì—…ë°ì´íŠ¸
        print(f"\n[Step 3/3] Updating company information...")
        if company_url:
            try:
                from models.interview import Company
                company = db.query(Company).filter(Company.id == company_id).first()
                if company:
                    company.company_url = company_url
                    db.commit()
                    print(f"  âœ“ Company URL updated")
                else:
                    print(f"  âš  Company {company_id} not found")
            except Exception as e:
                print(f"  âš  Failed to update company URL: {e}")
                # Continue without failing the entire request
        else:
            print(f"  - No company URL to update")

        print(f"\n{'='*60}")
        print(f"âœ“ Analysis completed successfully!")
        print(f"  - Competencies found: {len(analysis_result.get('competencies', []))}")
        print(f"{'='*60}\n")

        return CompetencyAnalysisResponse(
            competencies=analysis_result.get("competencies", []),
            category_weights=analysis_result.get("weights", {}),
            reasoning=analysis_result.get("reasoning", "")
        )

    except HTTPException:
        # Re-raise HTTP exceptions as-is
        raise
    except Exception as e:
        print(f"\n{'='*60}")
        print(f"âœ— Competency analysis failed: {e}")
        print(f"   Error type: {type(e).__name__}")
        print(f"{'='*60}\n")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to analyze competencies: {str(e)}"
        )
