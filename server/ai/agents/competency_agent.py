"""
Competency Agent (RAG í†µí•© ë²„ì „)
10ê°œ ì—­ëŸ‰ ë³‘ë ¬ í‰ê°€ + RAG ì§€ì›

ìˆ˜ì • ë‚´ìš©:
    1. LLM ì‘ë‹µ í›„ í•„ìˆ˜ í•„ë“œ ê²€ì¦
    2. key_observations ëˆ„ë½ ì‹œ ìë™ ìƒì„±
    3. RAG ê¸°ë°˜ ê´€ë ¨ segment ì„ ë³„ (85% í† í° ì ˆê°)
    4. Pydantic Structured Output ë„ì…
"""

import asyncio
from asyncio import Semaphore
import json
import hashlib
import os
from typing import Dict, Optional, List, Any
from datetime import datetime
from openai import AsyncOpenAI, RateLimitError, APIStatusError
from sqlalchemy.orm import Session
from pydantic import BaseModel, Field


# ==================== Pydantic Models ====================

class ConfidenceDetail(BaseModel):
    """ì‹ ë¢°ë„ ìƒì„¸ ì •ë³´"""
    level: str = Field(description="ì‹ ë¢°ë„ ìˆ˜ì¤€: high/medium/low")
    score: float = Field(ge=0, le=1, description="ì‹ ë¢°ë„ ì ìˆ˜ (0~1)")
    reasoning: str = Field(description="ì‹ ë¢°ë„ íŒë‹¨ ê·¼ê±°")


class SegmentEvidence(BaseModel):
    """ì„¸ê·¸ë¨¼íŠ¸ë³„ ì¦ê±° ìƒì„¸"""
    segment_id: int = Field(description="ì„¸ê·¸ë¨¼íŠ¸ ID")
    text: str = Field(description="ì¸ìš©ë¬¸ í…ìŠ¤íŠ¸")
    relevance_note: str = Field(description="ê´€ë ¨ì„± ì„¤ëª…")
    quality_score: float = Field(ge=0, le=1, description="ì¦ê±° í’ˆì§ˆ ì ìˆ˜")
    char_index: Optional[int] = Field(description="ë¬¸ì ì‹œì‘ ìœ„ì¹˜", default=None)
    char_length: Optional[int] = Field(description="ë¬¸ì ê¸¸ì´", default=None)


class PerspectivesDetail(BaseModel):
    """ë‹¤ê°ë„ í‰ê°€ ì •ë³´"""
    evidence_reasoning: str = Field(description="ê·¼ê±° ê¸°ë°˜ í‰ê°€ ë…¼ë¦¬")
    behavioral_indicators: List[str] = Field(description="ê´€ì°°ëœ í–‰ë™ ì§€í‘œ", default_factory=list)
    development_suggestions: List[str] = Field(description="ê°œë°œ ì œì•ˆì‚¬í•­", default_factory=list)
    evidence_details: List[SegmentEvidence] = Field(
        description="ì„¸ê·¸ë¨¼íŠ¸ë³„ ì¦ê±° ìƒì„¸ ëª©ë¡ (ë©´ì ‘ transcriptì˜ segment_id, ì¸ìš©ë¬¸, ê´€ë ¨ì„±, í’ˆì§ˆ ì ìˆ˜)",
        default_factory=list
    )


class SegmentReference(BaseModel):
    """ì„¸ê·¸ë¨¼íŠ¸ ì°¸ì¡° ì •ë³´"""
    segment_id: int = Field(description="ì„¸ê·¸ë¨¼íŠ¸ ID")
    relevance_score: float = Field(ge=0, le=1, description="ê´€ë ¨ì„± ì ìˆ˜")
    key_quote: str = Field(description="í•µì‹¬ ì¸ìš©ë¬¸")


class CompetencyEvaluationResult(BaseModel):
    """ì—­ëŸ‰ í‰ê°€ ê²°ê³¼ (Structured Output)"""
    competency_name: str = Field(description="ì—­ëŸ‰ ì´ë¦„ (ì˜ë¬¸)")
    overall_score: int = Field(ge=0, le=100, description="ì¢…í•© ì ìˆ˜ (0-100)")
    strengths: List[str] = Field(description="ê°•ì  ëª©ë¡ (2-5ê°œ)", min_length=1)
    weaknesses: List[str] = Field(description="ì•½ì  ëª©ë¡ (1-3ê°œ)", min_length=1)
    key_observations: List[str] = Field(description="í•µì‹¬ ê´€ì°°ì‚¬í•­ (3-5ê°œ)", min_length=3)
    perspectives: PerspectivesDetail = Field(description="ë‹¤ê°ë„ í‰ê°€")
    confidence: ConfidenceDetail = Field(description="í‰ê°€ ì‹ ë¢°ë„")
    segment_references: List[SegmentReference] = Field(
        description="ì°¸ì¡°í•œ ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´",
        default_factory=list
    )


class CompetencyAgent:
    """ì—­ëŸ‰ í‰ê°€ Agent (RAG í†µí•©)"""

    # í•„ìˆ˜ í•„ë“œ ì •ì˜
    REQUIRED_FIELDS = {
        "competency_name": str,
        "overall_score": int,
        "strengths": list,
        "weaknesses": list,
        "key_observations": list,
        "perspectives": dict,
        "confidence": dict
    }

    # RAG ê²€ìƒ‰ ì¿¼ë¦¬ (ì—­ëŸ‰ë³„)
    COMPETENCY_SEARCH_QUERIES = {
        # Common Competencies (5ê°œ)
        "achievement_motivation": "ëª©í‘œ ì„¤ì •, ìë°œì  ì‹œì‘, ë‚´ì  ë™ê¸°, ë¿Œë“¯í•œ ê²½í—˜, ì„±ì·¨ ìš•êµ¬, ë„ì „ ì¶”êµ¬, í”„ë¡œì íŠ¸ ì™„ìˆ˜, ìê¸°ì£¼ë„ì ",
        "growth_potential": "í•™ìŠµ ê²½í—˜, ì‹¤íŒ¨ë¡œë¶€í„° ë°°ì›€, ìƒˆë¡œìš´ ê¸°ìˆ  ìŠµë“, ìê¸°ê³„ë°œ, í”¼ë“œë°± ìˆ˜ìš©, ì„±ì¥ ë§ˆì¸ë“œ, ê°œì„  ë…¸ë ¥",
        "interpersonal_skill": "íŒ€ì›Œí¬, í˜‘ì—… ê²½í—˜, ê°ˆë“± í•´ê²°, ì»¤ë®¤ë‹ˆì¼€ì´ì…˜, ê´€ê³„ í˜•ì„±, ê³µê° ëŠ¥ë ¥, ì„¤ë“ë ¥, ë¦¬ìŠ¤ë‹",
        "organizational_fit": "ì¡°ì§ ë¬¸í™”, ê°€ì¹˜ê´€, ì—…ë¬´ ìŠ¤íƒ€ì¼, íŒ€ ì ì‘, íšŒì‚¬ ì„ íƒ ì´ìœ , ì—…ë¬´ í™˜ê²½, ì¡°ì§ ìƒí™œ",
        "problem_solving": "ë¬¸ì œ í•´ê²° ì‚¬ë¡€, ë…¼ë¦¬ì  ì‚¬ê³ , ì°½ì˜ì  ì ‘ê·¼, ë¶„ì„ ëŠ¥ë ¥, ë³µì¡í•œ ìƒí™© ëŒ€ì²˜, ì˜ì‚¬ê²°ì •",

        # Job Competencies (5ê°œ)
        "customer_journey_marketing": "ê³ ê° ì—¬ì •, VMD, ë§ˆì¼€íŒ… ì „ëµ, ë¸Œëœë“œ ê²½í—˜, ê³ ê° í–‰ë™, ë§¤ì¥ ìš´ì˜, ì‹œê°ì  ë¨¸ì²œë‹¤ì´ì§•",
        "md_data_analysis": "ë°ì´í„° ë¶„ì„, íŠ¸ë Œë“œ ë¶„ì„, ë§¤ì¶œ ë¶„ì„, ìƒí’ˆ ê¸°íš, íŒë§¤ ë°ì´í„°, ì¬ê³  ë¶„ì„, SKU ê´€ë¦¬, í”¼ë²— í…Œì´ë¸”",
        "seasonal_strategy_kpi": "ì‹œì¦Œ ì „ëµ, KPI ì„¤ì •, ëª©í‘œ ë‹¬ì„±, ì „ëµ ìˆ˜ë¦½, ì„±ê³¼ ì§€í‘œ, ë¹„ì¦ˆë‹ˆìŠ¤ ê³„íš, ì‹¤í–‰ë ¥",
        "stakeholder_collaboration": "ì´í•´ê´€ê³„ì í˜‘ì—…, ë¶€ì„œê°„ í˜‘ë ¥, í˜‘ìƒ, ì¡°ìœ¨, ì»¤ë®¤ë‹ˆì¼€ì´ì…˜, íŒŒíŠ¸ë„ˆì‹­",
        "value_chain_optimization": "ì†Œì‹±, ìƒì‚°, ìœ í†µ, ê³µê¸‰ë§, ì›ê°€ ì ˆê°, íš¨ìœ¨í™”, ë¬¼ë¥˜, ë²¤ë” ê´€ë¦¬",
    }
    
    def __init__(
        self,
        openai_client: AsyncOpenAI,
        max_concurrent: int = 5,
        max_retries: int = 5,
        db_session: Optional[Session] = None,
        use_rag: bool = False,
        rag_top_k: int = 8,
        use_feedback: bool = False,  # ğŸ†• í”¼ë“œë°± ë£¨í”„ í™œì„±í™”
        job_category: Optional[str] = None  # ğŸ†• ì§ë¬´ ì¹´í…Œê³ ë¦¬ (í”¼ë“œë°± ê²€ìƒ‰ìš©)
    ):
        self.client = openai_client
        self.model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        self.semaphore = Semaphore(max_concurrent)
        self.cache = {}
        self.max_retries = max_retries
        self.db_session = db_session
        self.use_rag = use_rag
        self.rag_top_k = rag_top_k
        self.use_feedback = use_feedback
        self.job_category = job_category
    
    def _get_cache_key(self, competency_name: str, transcript: Dict) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        transcript_str = json.dumps(transcript, sort_keys=True, ensure_ascii=False)
        transcript_hash = hashlib.md5(transcript_str.encode()).hexdigest()
        return f"{competency_name}:{transcript_hash}"
    
    
    def _validate_and_fix_response(
        self, 
        result: Dict, 
        competency_name: str
    ) -> Dict:
        """
        LLM ì‘ë‹µ ê²€ì¦ ë° í•„ìˆ˜ í•„ë“œ ë³´ê°•
        
        Args:
            result: LLMì´ ë°˜í™˜í•œ JSON
            competency_name: ì—­ëŸ‰ ì´ë¦„
        
        Returns:
            ê²€ì¦ ë° ë³´ê°•ëœ JSON
        """
        
        print(f"  [ê²€ì¦] {competency_name} ì‘ë‹µ ê²€ì¦ ì¤‘...")
        
        # 1. í•„ìˆ˜ í•„ë“œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        missing_fields = []
        for field, field_type in self.REQUIRED_FIELDS.items():
            if field not in result:
                missing_fields.append(field)
                print(f"    âš ï¸  í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
        
        
        # 2. key_observations ëˆ„ë½ ì‹œ ìë™ ìƒì„±
        if "key_observations" not in result or not result.get("key_observations"):
            print(f"    ğŸ”§ key_observations ìë™ ìƒì„± ì¤‘...")
            
            # strengths, weaknesses, perspectivesì—ì„œ í•µì‹¬ ê´€ì°° ì¶”ì¶œ
            key_obs = self._generate_key_observations(result, competency_name)
            result["key_observations"] = key_obs
            
            print(f"    âœ… key_observations ìƒì„± ì™„ë£Œ ({len(key_obs)}ê°œ)")
        else:
            print(f"    âœ… key_observations ì¡´ì¬ ({len(result['key_observations'])}ê°œ)")
        
        
        # 3. ë¹ˆ ë¦¬ìŠ¤íŠ¸ í•„ë“œ ê²½ê³ 
        if not result.get("strengths"):
            print(f"    âš ï¸  strengths ë¹„ì–´ìˆìŒ")
        
        if not result.get("weaknesses"):
            print(f"    âš ï¸  weaknesses ë¹„ì–´ìˆìŒ")
        
        
        # 4. ì ìˆ˜ ë²”ìœ„ ê²€ì¦
        score = result.get("overall_score", 0)
        if not (0 <= score <= 100):
            print(f"    âš ï¸  overall_score ë²”ìœ„ ì˜¤ë¥˜: {score} â†’ 50ìœ¼ë¡œ ì¡°ì •")
            result["overall_score"] = 50
        
        
        return result
    
    
    def _generate_key_observations(
        self,
        result: Dict,
        competency_name: str
    ) -> list:
        """
        key_observations ìë™ ìƒì„±

        ì „ëµ:
            1. strengths/weaknessesì—ì„œ ìƒìœ„ 3ê°œ ì¶”ì¶œ
            2. perspectives.evidence_reasoningì—ì„œ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ
            3. ìµœì†Œ 3ê°œ ë³´ì¥
        """

        key_obs = []

        # 1. Strengthsì—ì„œ ì¶”ì¶œ (ìƒìœ„ 2ê°œ)
        strengths = result.get("strengths", [])
        if strengths:
            key_obs.extend(strengths[:2])


        # 2. Weaknessesì—ì„œ ì¶”ì¶œ (ìƒìœ„ 1ê°œ)
        weaknesses = result.get("weaknesses", [])
        if weaknesses:
            key_obs.append(weaknesses[0])


        # 3. Evidence reasoningì—ì„œ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ
        perspectives = result.get("perspectives", {})
        evidence_reasoning = perspectives.get("evidence_reasoning", "")

        if evidence_reasoning:
            # "ë”°ë¼ì„œ", "ì „ë°˜ì ìœ¼ë¡œ" ê°™ì€ í‚¤ì›Œë“œ ë’¤ ë¬¸ì¥ ì¶”ì¶œ
            import re
            # "ë”°ë¼ì„œ Xì  ì‚°ì •" ê°™ì€ ê²°ë¡  ë¬¸ì¥ ì°¾ê¸°
            conclusion_match = re.search(r'(ë”°ë¼ì„œ|ì „ë°˜ì ìœ¼ë¡œ|ì¢…í•©í•˜ë©´)[^.]+\.', evidence_reasoning)
            if conclusion_match:
                conclusion = conclusion_match.group(0).strip()
                if conclusion not in key_obs:
                    key_obs.append(conclusion)


        # 4. ìµœì†Œ 3ê°œ ë³´ì¥ (ë¶€ì¡±í•˜ë©´ ê¸°ë³¸ ë©”ì‹œì§€ ì¶”ê°€)
        if len(key_obs) < 3:
            score = result.get("overall_score", 0)

            # ì ìˆ˜ ëŒ€ì—­ë³„ ê¸°ë³¸ ê´€ì°°
            if score >= 75:
                key_obs.append(f"{competency_name} ì—­ëŸ‰ì´ ì‹ ì… ê¸°ì¤€ ìš°ìˆ˜í•œ ìˆ˜ì¤€")
            elif score >= 60:
                key_obs.append(f"{competency_name} ì—­ëŸ‰ì´ ì‹ ì… ê¸°ì¤€ ì–‘í˜¸í•œ ìˆ˜ì¤€")
            elif score >= 50:
                key_obs.append(f"{competency_name} ì—­ëŸ‰ì´ ì‹ ì… ê¸°ì¤€ í‰ê·  ìˆ˜ì¤€")
            else:
                key_obs.append(f"{competency_name} ì—­ëŸ‰ì´ ì‹ ì… ê¸°ì¤€ ë¯¸í¡í•œ ìˆ˜ì¤€")


        # 5. ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 5ê°œë¡œ ì œí•œ
        key_obs = list(dict.fromkeys(key_obs))[:5]

        return key_obs


    async def _filter_transcript_with_rag(
        self,
        competency_name: str,
        transcript: Dict,
        session_id: int
    ) -> Dict:
        """
        RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ segmentë§Œ í•„í„°ë§í•œ transcript ë°˜í™˜

        Args:
            competency_name: ì—­ëŸ‰ ì´ë¦„
            transcript: ì „ì²´ transcript
            session_id: ì„¸ì…˜ ID (DB ì¡°íšŒìš©)

        Returns:
            í•„í„°ë§ëœ transcript (ê´€ë ¨ segmentë§Œ í¬í•¨)
        """
        if not self.use_rag or not self.db_session:
            return transcript

        try:
            # RAG ê²€ìƒ‰ ì¿¼ë¦¬ ê°€ì ¸ì˜¤ê¸°
            search_query = self.COMPETENCY_SEARCH_QUERIES.get(
                competency_name,
                f"{competency_name} ê´€ë ¨ í–‰ë™ ì‚¬ë¡€"
            )

            # RAG ê²€ìƒ‰ ì‹¤í–‰
            from services.rag_embedding_service import search_relevant_transcripts

            relevant_transcripts = await search_relevant_transcripts(
                db=self.db_session,
                session_id=session_id,
                query_text=search_query,
                top_k=self.rag_top_k,
                similarity_threshold=0.3  # ìµœì†Œ 30% ìœ ì‚¬ë„
            )

            if not relevant_transcripts:
                print(f"  âš ï¸  [{competency_name}] RAG ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ â†’ ì „ì²´ transcript ì‚¬ìš©")
                return transcript

            # ê´€ë ¨ segment ID ì¶”ì¶œ
            relevant_segment_ids = set([t["turn"] for t in relevant_transcripts])

            # Transcript í•„í„°ë§
            filtered_segments = [
                seg for seg in transcript.get("segments", [])
                if seg.get("segment_order") in relevant_segment_ids
            ]

            # í•„í„°ë§ëœ transcript ìƒì„±
            filtered_transcript = {
                **transcript,
                "segments": filtered_segments,
                "rag_metadata": {
                    "competency_name": competency_name,
                    "search_query": search_query,
                    "original_segment_count": len(transcript.get("segments", [])),
                    "filtered_segment_count": len(filtered_segments),
                    "token_reduction_rate": 1 - (len(filtered_segments) / max(len(transcript.get("segments", [])), 1))
                }
            }

            print(f"  ğŸ” [{competency_name}] RAG í•„í„°ë§: {len(transcript.get('segments', []))}ê°œ â†’ {len(filtered_segments)}ê°œ segment")

            return filtered_transcript

        except Exception as e:
            print(f"  âš ï¸  [{competency_name}] RAG í•„í„°ë§ ì‹¤íŒ¨: {e} â†’ ì „ì²´ transcript ì‚¬ìš©")
            return transcript


    async def _get_feedback_examples(
        self,
        competency_name: str,
        transcript_summary: str
    ) -> List[Dict]:
        """
        ğŸ†• V2: Few-shot examplesë¡œ ì‚¬ìš©í•  í”¼ë“œë°± ê²€ìƒ‰

        Args:
            competency_name: ì—­ëŸ‰ ì´ë¦„
            transcript_summary: í˜„ì¬ íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ìš”ì•½

        Returns:
            Few-shot examples ë¦¬ìŠ¤íŠ¸
        """
        if not self.use_feedback or not self.db_session or not self.job_category:
            return []

        try:
            from services.feedback.feedback_manager import FeedbackManager

            feedback_manager = FeedbackManager(db=self.db_session, openai_client=self.client)

            # ğŸ†• V2: Dynamic threshold ì‚¬ìš©
            feedbacks = await feedback_manager.get_relevant_feedback(
                job_category=self.job_category,
                competency_name=competency_name,
                current_context=transcript_summary,
                top_k=2,  # Few-shotì€ 2ê°œë©´ ì¶©ë¶„
                similarity_threshold=0.5,
                use_dynamic_threshold=True  # ğŸ†• ë™ì  ì¡°ì •
            )

            if feedbacks:
                print(f"  ğŸ’¡ [{competency_name}] Few-shot examples: {len(feedbacks)}ê°œ")

            return feedbacks

        except Exception as e:
            print(f"  âš ï¸  [{competency_name}] í”¼ë“œë°± ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def _build_messages_with_feedback(
        self,
        prompt: str,
        feedbacks: List[Dict]
    ) -> List[Dict]:
        """
        ğŸ†• V2: Few-shot promptingìœ¼ë¡œ ë©”ì‹œì§€ êµ¬ì„±

        Args:
            prompt: ì›ë³¸ í”„ë¡¬í”„íŠ¸
            feedbacks: í”¼ë“œë°± ëª©ë¡

        Returns:
            Few-shot examplesê°€ í¬í•¨ëœ messages
        """
        messages = [
            {
                "role": "system",
                "content": "You are an expert HR evaluator. Learn from past corrections to improve your assessments."
            }
        ]

        # ğŸ†• V2: Few-shot examples ì¶”ê°€ (ìµœëŒ€ 2ê°œ)
        for fb in feedbacks[:2]:
            # AIì˜ ì˜ëª»ëœ í‰ê°€ ì˜ˆì‹œ
            messages.append({
                "role": "user",
                "content": f"Evaluate competency. Context: Similar to cases where AI scored {fb['ai_score']} but was corrected."
            })
            messages.append({
                "role": "assistant",
                "content": f"Initial assessment: {fb['ai_score']} points. {fb['mistake_summary']}"
            })

            # HRì˜ êµì • í”¼ë“œë°±
            messages.append({
                "role": "user",
                "content": f"HR Correction: {fb['correction_guideline']}. Actual score should be {fb['human_score']}."
            })
            messages.append({
                "role": "assistant",
                "content": f"Understood. I will adjust my evaluation approach. Corrected score: {fb['human_score']} points."
            })

        # ì‹¤ì œ í‰ê°€ í”„ë¡¬í”„íŠ¸
        messages.append({
            "role": "user",
            "content": prompt
        })

        return messages
    
    
    async def evaluate(
        self,
        competency_name: str,
        competency_display_name: str,
        competency_category: str,
        prompt: str,
        transcript: Dict,
        session_id: Optional[int] = None
    ) -> Dict:
        """ì—­ëŸ‰ í‰ê°€ ì‹¤í–‰ (V2: Few-shot Prompting + ì¡°ê±´ë¶€ ìºì‹œ)"""

        # ğŸ†• V2: í”¼ë“œë°± ì‚¬ìš© ì‹œ ìºì‹œ ë¹„í™œì„±í™”
        cache_enabled = not self.use_feedback

        # ìºì‹œ í™•ì¸
        if cache_enabled:
            cache_key = self._get_cache_key(competency_name, transcript)
            if cache_key in self.cache:
                print(f"[ìºì‹œ íˆíŠ¸] {competency_name}")
                return self.cache[cache_key]

        # Rate Limiting
        async with self.semaphore:
            print(f"[í‰ê°€ ì‹œì‘] {competency_name}")

            # RAG í•„í„°ë§ (ì„ íƒì )
            if self.use_rag and session_id:
                transcript = await self._filter_transcript_with_rag(
                    competency_name=competency_name,
                    transcript=transcript,
                    session_id=session_id
                )

            # ğŸ†• V2: Few-shot examples ê°€ì ¸ì˜¤ê¸°
            feedback_examples = []
            if self.use_feedback:
                # íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ìš”ì•½ ìƒì„± (ê°„ë‹¨íˆ ì²« 3ê°œ segment ìš”ì•½)
                transcript_summary = ""
                if transcript.get("segments"):
                    segments = transcript["segments"][:3]
                    transcript_summary = " ".join([
                        f"{seg.get('question_text', '')} {seg.get('answer_text', '')}"
                        for seg in segments
                    ])[:500]  # 500ìë¡œ ì œí•œ

                feedback_examples = await self._get_feedback_examples(
                    competency_name=competency_name,
                    transcript_summary=transcript_summary
                )

            # ğŸ†• V2: Few-shot promptingìœ¼ë¡œ ë©”ì‹œì§€ êµ¬ì„±
            if feedback_examples:
                messages = self._build_messages_with_feedback(prompt, feedback_examples)
            else:
                # í”¼ë“œë°± ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹
                messages = [
                    {
                        "role": "system",
                        "content": "You are an expert HR evaluator. Evaluate competencies based on interview transcripts."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]

            try:
                # OpenAI í˜¸ì¶œ (ì¬ì‹œë„ í¬í•¨) - Pydantic Structured Output
                for attempt in range(self.max_retries):
                    try:
                        # ğŸ†• V2: Few-shot messages ì‚¬ìš©
                        response = await self.client.beta.chat.completions.parse(
                            model=self.model,
                            messages=messages,
                            temperature=0.0,
                            max_tokens=4000,
                            response_format=CompetencyEvaluationResult
                        )

                        # Pydantic ëª¨ë¸ë¡œ ìë™ íŒŒì‹±ë¨
                        parsed_result = response.choices[0].message.parsed

                        # Pydantic ëª¨ë¸ì„ Dictë¡œ ë³€í™˜
                        result = parsed_result.model_dump()

                        # ğŸ†• Confidence ê²€ì¦ (ë‚®ìœ¼ë©´ ì¬ì‹œë„)
                        confidence_score = result.get("confidence", {}).get("score", 1.0)
                        confidence_level = result.get("confidence", {}).get("level", "high")

                        if confidence_score < 0.6 and attempt < self.max_retries - 1:
                            print(f"  âš ï¸  [{competency_name}] Low confidence ({confidence_score:.2f}) - retrying ({attempt+1}/{self.max_retries})")
                            # ì¬ì‹œë„ë¥¼ ìœ„í•´ continue
                            await asyncio.sleep(1)
                            continue

                        # ë©”íƒ€ ì •ë³´ ì¶”ê°€
                        result["competency_name"] = competency_name
                        result["competency_display_name"] = competency_display_name
                        result["competency_category"] = competency_category
                        result["evaluated_at"] = datetime.now().isoformat()

                        # ğŸ†• V2: ì¡°ê±´ë¶€ ìºì‹± (í”¼ë“œë°± ì‚¬ìš© ì‹œ ìºì‹œ ì•ˆí•¨)
                        if cache_enabled:
                            cache_key = self._get_cache_key(competency_name, transcript)
                            self.cache[cache_key] = result

                        feedback_note = f" (Few-shot: {len(feedback_examples)}ê°œ)" if feedback_examples else ""
                        print(f"[í‰ê°€ ì™„ë£Œ] {competency_name}: {result.get('overall_score', 0)}ì  (Confidence: {confidence_level}, {confidence_score:.2f}){feedback_note}")

                        return result
                        
                    except RateLimitError as e:
                        await self._handle_rate_limit(e, attempt, competency_name)
                        continue
                    except APIStatusError as e:
                        if e.status_code == 429:
                            await self._handle_rate_limit(e, attempt, competency_name)
                            continue
                        raise
                    except json.JSONDecodeError as e:
                        if attempt < self.max_retries - 1:
                            print(f"[ì¬ì‹œë„ {attempt+1}/{self.max_retries}] {competency_name}: JSON íŒŒì‹± ì˜¤ë¥˜ â†’ ë°±ì˜¤í”„ í›„ ì¬ì‹œë„")
                            await asyncio.sleep(1 + attempt)
                        else:
                            raise
                    except Exception as e:
                        if attempt < self.max_retries - 1:
                            print(f"[ì¬ì‹œë„ {attempt+1}/{self.max_retries}] {competency_name}: {e}")
                            await asyncio.sleep(1 + attempt)
                        else:
                            raise
                            
            except Exception as e:
                raise RuntimeError(f"[{competency_name}] í‰ê°€ ì‹¤íŒ¨: {e}")

    async def _handle_rate_limit(self, error: Exception, attempt: int, competency_name: str):
        """429 ì˜¤ë¥˜ ëŒ€ì‘: retry-after ë˜ëŠ” ì§€ìˆ˜ ë°±ì˜¤í”„ ê¸°ë°˜ ëŒ€ê¸°"""
        retry_after = None
        response = getattr(error, "response", None)
        if response:
            retry_after = response.headers.get("retry-after") or response.headers.get("Retry-After")
        if retry_after:
            try:
                wait_seconds = float(retry_after)
            except ValueError:
                wait_seconds = None
        else:
            wait_seconds = None
        if wait_seconds is None:
            import re
            match = re.search(r"try again in ([0-9.]+)s", str(error))
            if match:
                wait_seconds = float(match.group(1))
        if wait_seconds is None:
            wait_seconds = min(30, 2 ** attempt * 2)
        print(f"[ëŒ€ê¸°] {competency_name} rate limit ê°ì§€ â†’ {wait_seconds:.1f}s í›„ ì¬ì‹œë„ ({attempt+1}/{self.max_retries})")
        await asyncio.sleep(wait_seconds)


async def evaluate_all_competencies(
    agent: CompetencyAgent,
    transcript: Dict,
    prompts: Dict[str, str],
    session_id: Optional[int] = None,
    job_competencies: Optional[List[Dict[str, Any]]] = None
) -> Dict[str, Dict]:
    """10ê°œ ì—­ëŸ‰ ë°°ì¹˜ í‰ê°€ (RAG ì§€ì› + ë™ì  ì—­ëŸ‰)"""

    print("=" * 60)
    print("10ê°œ ì—­ëŸ‰ ë°°ì¹˜ í‰ê°€ ì‹œì‘")
    if agent.use_rag:
        print(f"  ğŸ” RAG ëª¨ë“œ í™œì„±í™” (Top-K: {agent.rag_top_k})")
    print("=" * 60)

    # Common Competencies (5ê°œ ê³ ì •)
    common_competency_configs = [
        ("achievement_motivation", "ì„±ì·¨/ë™ê¸° ì—­ëŸ‰", "common"),
        ("growth_potential", "ì„±ì¥ ì ì¬ë ¥", "common"),
        ("interpersonal_skill", "ëŒ€ì¸ê´€ê³„ ì—­ëŸ‰", "common"),
        ("organizational_fit", "ì¡°ì§ ì í•©ì„±", "common"),
        ("problem_solving", "ë¬¸ì œí•´ê²°ë ¥", "common"),
    ]

    # Job Competencies (5ê°œ ë™ì )
    job_competency_configs = []

    if job_competencies and len(job_competencies) >= 5:
        # JDì—ì„œ ì¶”ì¶œí•œ ì—­ëŸ‰ ì‚¬ìš©
        print(f"  âœ“ Using {len(job_competencies)} job competencies from JD")
        for i, comp in enumerate(job_competencies[:5]):  # ìµœëŒ€ 5ê°œ
            # competency êµ¬ì¡°: {"name": "...", "category": "...", "score": ..., "description": "..."}
            comp_name = comp.get("name", f"job_comp_{i+1}")
            # ì˜ì–´ keyë¡œ ë³€í™˜ (snake_case)
            comp_key = comp_name.lower().replace(" ", "_").replace("(", "").replace(")", "").replace("&", "and")
            job_competency_configs.append((
                comp_key,
                comp_name,
                "job"
            ))
    else:
        # ê¸°ë³¸ ì§ë¬´ ì—­ëŸ‰ ì‚¬ìš© (Fallback)
        print("  âš  Using default job competencies (no JD competencies found)")
        job_competency_configs = [
            ("customer_journey_marketing", "ê³ ê° ì—¬ì • ì„¤ê³„ ë° VMDÂ·ë§ˆì¼€íŒ… í†µí•© ì „ëµ", "job"),
            ("md_data_analysis", "ë§¤ì¶œÂ·íŠ¸ë Œë“œ ë°ì´í„° ë¶„ì„ ë° ìƒí’ˆ ê¸°íš", "job"),
            ("seasonal_strategy_kpi", "ì‹œì¦Œ ì „ëµ ìˆ˜ë¦½ ë° ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œí•´ê²°", "job"),
            ("stakeholder_collaboration", "ìœ ê´€ë¶€ì„œ í˜‘ì—… ë° ì´í•´ê´€ê³„ì í˜‘ìƒ", "job"),
            ("value_chain_optimization", "ì†Œì‹±Â·ìƒì‚°Â·ìœ í†µ ë°¸ë¥˜ì²´ì¸ ìµœì í™”", "job"),
        ]

    # ì „ì²´ ì—­ëŸ‰ ì„¤ì • (ê³µí†µ 5ê°œ + ì§ë¬´ 5ê°œ)
    competency_configs = common_competency_configs + job_competency_configs

    # ë³‘ë ¬ í‰ê°€ ì‹¤í–‰
    tasks = [
        agent.evaluate(name, display, category, prompts[name], transcript, session_id)
        for name, display, category in competency_configs
    ]
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    print("=" * 60)
    print("ë°°ì¹˜ í‰ê°€ ì™„ë£Œ")
    print("=" * 60)
    
    # ê²°ê³¼ ë§¤í•‘
    result_dict = {}
    for (name, _, _), result in zip(competency_configs, results):
        if isinstance(result, Exception):
            print(f"[ì˜¤ë¥˜] {name}: {str(result)}")
            result_dict[name] = {
                "error": str(result),
                "overall_score": 0,
                "confidence": {
                    "overall_confidence": 0.3
                },
                "key_observations": [f"{name} í‰ê°€ ì‹¤íŒ¨"]  # ğŸ†• ì—ëŸ¬ ì‹œì—ë„ í•„ë“œ ë³´ì¥
            }
        else:
            result_dict[name] = result
    
    return result_dict
